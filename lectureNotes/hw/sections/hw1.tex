\documentclass[../main.tex]{subfiles}



%% Local Macros and packages: add any of your own definitions here.

\begin{document}

% Homework number, your name and NetID, and optionally, the names/NetIDs of anyone you collaborated with. If you did not collaborate with anybody, leave the last parameter empty.

\section*{Problem (1a)}


Define $X_{i}$ as the value chosen in the $i$th iteration. It follows that

\[
    X = \frac{1}{k}\sum_{i=1}^{k} X_{i}
\]

From this, we can compute $\Ex{X}$ and $\var{X}$.
\begin{align*}
    \Ex{X} &= \frac{1}{k} \Ex{\sum_{i=1}^{k} X_{i}} \\
    &= \frac{1}{k} \sum_{i=1}^{k} \Ex{X_{i}} \\
    &= \frac{1}{k} \sum_{i=1}^{k} \Ex{X_{i}} \\
    &= \frac{1}{k} \sum_{i=1}^{k} \sum_{j=1}^{n}z_{j} \frac{1}{n} \\
    &= \frac{1}{k} \sum_{i=1}^{k} \sum_{j=1}^{n}z_{j} \frac{1}{n} \\
    &= \frac{1}{k} \sum_{i=1}^{k} \alpha \\
    &= \alpha \\
    \\
    \var{X} &= \var{\frac{1}{k}\sum_{i=1}^{k} X_{i}} \\
    \intertext{Observe that each $X_{i}$ is independent of another. Thus, the variance
operator distributes:}
    \var{X} &= \frac{1}{k^2}\sum_{i=1}^{k} \var{X_{i}} \\
    \var{X} &= \frac{1}{k^2}\sum_{i=1}^{k} \var{X_{i}} \\
    \var{X_{i}} &= \Ex{X_{i}^2} - \Ex{X_{i}}^2 \\
    \var{X_{i}} &= \sum_{j=1}^{n}\frac{1}{n}z_{j}^2 - \alpha^2 \\
    \var{X_{i}} &= \sum_{j=1}^{n}\left(\frac{z_{j}^2 - \alpha^2}{n}\right) \\
    \implies \var{X} &= \frac{1}{k}\sum_{j=1}^{n} \left(\frac{z_{j}^2 - \alpha^2}{n}\right) \\
\end{align*}

Since $\var{X}$ is finite, we can apply Chebyshev's inequality:

\begin{align*}
    & \prob{\abs{X - \alpha} \geq \e} \leq \frac{\sigma_{x}^2}{\e^2} \\
    & \prob{\abs{X - \alpha} \geq \e} \leq \frac{\frac{1}{k}\sum_{j=1}^{n} \left(\frac{z_{j}^2 - \alpha^2}{n}\right)}{\e^2} \\
    & \prob{\abs{X - \alpha} \geq \e} \leq \frac{\sum_{j=1}^{n} \left(\frac{z_{j}^2 - \alpha^2}{n}\right)}{k \e^2} \\
    &\leq \frac{\delta \e^2 \sum_{j=1}^{n} \left(\frac{z_{j}^2 - \alpha^2}{n}\right)}{(b-a)^2 \e^2} \\
    &\leq \frac{\delta \sum_{j=1}^{n} \left(\frac{z_{j}^2 - \alpha^2}{n}\right)}{(b-a)^2 } \\
    \intertext{To complete the proof, it suffices to show that:} \\
    &\frac{ \sum_{j=1}^{n}\left(z_{j}^2 - \alpha^2\right)}{n(b-a)^2 } \leq 1 \\
    \intertext{Observe that the LHS is precisely} \\
    &\frac{\var{X_{i}}}{(b-a)^2 } \\
    &= \frac{\sum_{i=1}^{n}\left( z_{i} - \alpha \right)^2}{(b-a)^2 } \\
    &\leq 1 \\
    \qed
\end{align*}

\section*{Problem (1b)}

A known generalization of the Chernoff inequality says that if $X_{i} \in [a_{i}, b_{i}]$, and if $X = \sum_{i=1}^{n}X_{i}$, then

\[
    \prob{\abs{X - \mu} \geq \Delta} \leq 2\exp\left\{ -\frac{2 \Delta^2}{\sum_{i=1}^{n}(b_{i}-a_{i})^2}  \right\}
\]

Letting $\Delta = \e, \mu = \alpha, a_{i} = a, b_{i} = b, n = k$, our bound is

\[
    \prob{\abs{X - \alpha} \geq \e} \leq 2\exp\left\{ -\frac{2 \e^2}{\sum_{i=1}^{k}(b-a)^2}  \right\}
\]

To find our desired value for $c$, we can bound the RHS to the right by $\delta$ and find an appropriate value for $c$

\begin{align*}
2\exp\left\{ -\frac{2 \e^2}{\sum_{i=1}^{k}(b-a)^2}  \right\} \leq \delta  \\
\exp\left\{ -\frac{2 \e^2}{\sum_{i=1}^{k}(b-a)^2}  \right\} \leq \frac{\delta}{2} \\
 -\frac{2 \e^2}{\sum_{i=1}^{k}(b-a)^2} \leq \ln \left(\frac{\delta}{2}\right) \\
 -\frac{2 \e^2}{k(b-a)^2} \leq \ln \left(\frac{\delta}{2}\right) \\
 \intertext{Since we care about the regime when $\delta < 1$, the right hand side can be assumed negative, and so we can perform the following inversion:} \\
 -\frac{k(b-a)^2}{2 \e^2} \geq 1 \bigg/ \ln \left(\frac{\delta}{2}\right) \\
 k(b-a)^2 \leq -2 \e^2 \bigg/ \ln \left(\frac{\delta}{2}\right) \\
 k(b-a)^2 \leq 2 \e^2 \bigg/ \ln \left(\frac{2}{\delta}\right) \\
 k \leq \frac{2 \e^2}{(b-a)^2 \ln \left(\frac{2}{\delta}\right)} \\
 \frac{1}{k} \geq \frac{(b-a)^2 \ln \left(\frac{2}{\delta}\right)}{2 \e^2} \\
\end{align*}

So let $c = 1/2$. $\langle$

\pp{2}

\begin{algo}
    \xs{Sort}(Array): \+\\
    bst \leftarrow \xs{makeBBST}(Array) \\
    \redcomment{Store rank the number of elements to a given node's left} \\
    \redcomment{It takes $n \log n$ to make a BST } \\
    \redcomment{and $\log n$ to determine the rank of an element chosen from the array} \\
    $x$ \leftarrow Random number from $1$ to $n$ \\
    While bst$[x]$.rank < $n/4$ or bst$[x]$.rank > $3n/4$: \+\\
    $x$ \leftarrow Random number from $1$ to $n$ \-\\
    left, right \leftarrow \xs{Partition}(Array) \\
    \xs{Sort}(left) \\
    \xs{Sort}(right) \\
\end{algo}


The run time of this algorithm is the sum of the amount of time taken to choose a pivot at each stage, and the amount of comparisons made. Since a successful pivot produces (in reference to lecture 3), a lucky partition at every depth of the recursion, the amount of comparisons is $O(n \log n)$. We need to establish the cost required to choose pivots, however:

We first observe that there are, at the most, $\log_{4/3}(n)$ recursive depths (see lecture 3). Let us examine one level of the recursion, in order to make an expression for the total running time $X$. At level $j$, there are, at the most, $2^j$ arrays being sorted and each has size at most $n(3/4)^j$. Each of these arrays has had a BST built for it. Moreover, each of these arrays keeps selecting a random number from its portion of the original array, until the obtained number has ``lucky'' rank. The latter process describes a geometric distribution with probability $1/2$ precisely. At the $j$th level, we index each of the geometric variables by $i$ from $i=1$ to $i=2^j$ with $X_{j,i}$. Putting all of this together, the cost at one level is

\[
    \log(n (3/4)^j) \sum_{i=1}^{2^j}(X_{ji}) + \left[ n(3/4)^j \log\left( n(3/4)^j \right) \right](2^j)
\]

Sum this over all levels but the last one to get:
\[
    \sum_{j=0}^{\log_{4/3}(n) - 1}\log(n (3/4)^j) \sum_{i=1}^{2^j}(X_{ji}) + \left[ n(3/4)^j \log\left( n(3/4)^j \right) \right](2^j)
\]

%\begingroup
%\addtolength{\jot}{1em}
%\begin{align*}
    %H f = \frac{\partial }{\partial w}\nabla f = \frac{\partial }{\partial w} \frac{-1}{n}\sum_{i \in [n]}^{}\frac{y^{i}(x^{i})}{\left(1 + \exp(y^{i}w^Tx^{i})\right)}   \\ 
%\end{align*}
%\endgroup




%\newpage \nocite{*}
%\bibliographystyle{ims}
%\bibliography{citations}

\end{document}
