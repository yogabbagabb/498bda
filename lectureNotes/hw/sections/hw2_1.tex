\documentclass[../main.tex]{subfiles}



%% Local Macros and packages: add any of your own definitions here.

\begin{document}

% Homework number, your name and NetID, and optionally, the names/NetIDs of anyone you collaborated with. If you did not collaborate with anybody, leave the last parameter empty.
\homework
    {0}
    {Aahan Agrawal (agrawl10)}
    {Some Person (sperson2), Another Person (aperson3)}

\section*{Problem (1)}

(a) \\

Note that both $H_{1}$ and $H_{-1}$ are parallel hyperplanes, since their normal vectors are equal. It follows that to find the distance between both planes, given a point $x_1$ lying on $H_{1}$, the closest point on $H_{-1}$ to $x_1$ is the point $x_2$ obtained from intersecting the line $\{x_1 + wt | t \in \R\}$ with $H_{-1}$. That is $x_2 = x_1 + wt$. We seek to find $\norm{wt}$. Note that $w^Tx_1 = 1$ and $w^Tx_2 = -1$. Hence:

\begin{align*}
    x_2 - x_1 = wt \\
    \implies w^T(x_2 - x_1) = (w^tw)t \\
    \implies -2 = \norm{w}^2 t \\
    \implies t = \frac{-2}{\norm{w}^2} \\
    \implies \norm{wt} = \norm{\left[ w\left( \frac{-2}{\norm{w}^2} \right) \right]} \\
    = \frac{2}{\norm{w}}
\end{align*}

(b) \\

We prove this by contradiction. Suppose that there is a better maximum margin classifier $w' \not = w$ for the set $(X^{\ast}, Y^{\ast})$. We will show that $w'$ is also the maximum margin
classifier for $(X,Y)$ then, which is a contradiction, since we assumed that $w$ was the maximum margin classifier and the maximum margin classifier is unique \footnote{Solving for the maximum margin classifier in the separable case is a strictly convex problem and, hence, the minimizer to the problem is unique.} \\

By assumption, since $w'$ is a better classifier for $(X^{\ast}, Y^{\ast})$, \\

\[
    y^{i}(w')Tx^{i} \geq 1 \qquad \text{for all $i \in \mc{N}$ and} \tag{$\gamma$}
\]

\[
    \frac{2}{\norm{w'}} \geq \frac{2}{\norm{w}}
\]


Thus, to prove that $w'$ is the maximum margin classifier for $(X,Y)$, we need to show that \\

\[
    y^{j}(w')^Tx^{j} \geq 1 \qquad \text{For all $j \not \in \mc{N}$}
\]

Let $H_{1} = \{(x,1) \in (\R^n, \R^n) | (1)w^Tx = 1\}$. Without loss of generality, consider any $(x^{j}, y^{j})$ for $j \not \in \mc{N}$ such that $y^{j} = 1$. There exists some $(z,1) \in H_{1}$ such that $z + \alpha w = x^{j}$, where $\alpha > 0$. Thus \\

\begin{align*}
y^j(w')^Tx^j = y^j\alpha(w')^T(z + w) \\
= y^j(w')^Tz + y^j\alpha (w')^Tw \\
\geq 1 + y^j\alpha (w')^Tw \\
= 1  + \alpha(w')^Tw \\
\end{align*}

It suffices to show that $(w')^Tw \geq 0$, to prove that $(x^{j}, y^{j})$ is correctly classified using $w'$.  \\

Observe that for any $\beta > 0$, we must have that $\beta w$ is classified using the maximum margin classifier under $w$, which we label $\mc{A}_w$, as having label $1$. For if $q$ is the label of $\beta w$, then \\

\[
    \mc{A}_w(\beta w) = q(w^T(\beta w)) > 0 \implies q = 1 \\
\]

Thus, in particular, there exists some $\beta' > 0$ such that $\mc{A}_w(\beta' w) = 1$, meaning that $\beta' w$ lies on $H_1$ and we must have, by construction, $(w')^T (\beta' w) > 0 \implies (w')^Tw \geq 0$. \\

Thus $w'$ correctly classifies even $(x^{j}, y^{j})$. Since $j \not \in \mc{N}$ was arbitrary, $\mc{A}_w'$ correctly classifies all $j \not \in \mc{N}$. $\mc{A}_w'$ already correctly classfied those $(x^i,y^i) \in \mc{N}$ and $\frac{2}{\norm{w'}} \geq \frac{2}{\norm{w}}$, so we conclude that $w'$ is a better classifier than $w$ for $(X,Y)$, which is a contradiction.



\end{document}
