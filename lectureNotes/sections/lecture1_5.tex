\documentclass[../main.tex]{subfiles}
\renewcommand{\bm}[1]{\mathbf{#1}}

\begin{document}

\section{Lecture 1}{Inequalities and Sampling}

\begin{theorem}{Markov's Inequality}
    $\bb{P}(X \geq t) t \leq \bb{E}[X]$. Easy to see why. Note that this is only true
    for non-negative random variables.
    \label{thm:markov}
\end{theorem}

\begin{remark}
    This inequality is sharp, meaning that it cannot be further tightened. For example, let $X$ be a random variable that
    takes on $1$ with probability $1 - a$ and $0$ with probability $0$. Then $\bb{E}[X] = (1)(1 - a) + 0(a) \geq (1)(\bb{P}[X \geq 1]) = 1-a$. In this case, the $\geq$ is, in fact, an equality $=$.
\end{remark}

\begin{theorem}{Chebyshev's Inequality}
    It is UNRESOLVED why we need for $\text{Var}(X) < \infty$
    \[
        \bb{P}[(X - \bb{E}[X])^2 \geq t^2 \sigma_{x}^2] \leq \frac{1}{t^2}
    \]

    Note that this statement is also that

    \[
        \bb{P}[\abs{X - \bb{E}[X]} \geq t] = \bb{P}[(X - \bb{E}[X])^2 \geq t^2 ] \leq \frac{\sigma_x^2}{t^2}
    \]
\end{theorem}



\begin{proof}
    \begin{align*}
        \bb{P}(X \geq t) t &\leq \bb{E}[X]    \\
        \bb{P}(X^2 \geq t^2) t^2 &\leq \bb{E}[X^2]    \\
        \bb{P}((X - \mu_x)^2 \geq (t\sigma_x)^2) (t\sigma_x)^2 &\leq \bb{E}[(X - \mu_x)^2]    \\
        \bb{P}((X - \mu_x)^2 \geq (t\sigma_x)^2)  &\leq \frac{\bb{E}[(X - \mu_x)^2]}{(t\sigma_x)^2} = \frac{1}{t^2}    \\
    \end{align*}
\end{proof}

\section{Lecture 2}{Introduction to Randomization}
UNRESOLVED (MISSING MATRIX MULT AND TRIANGLES)

\begin{problem}{Expected Vertex Count}
    Suppose a graph has $n$ vertices and we randomly delete each vertex with probability $1/2$. The expected number of vertices is: \\

    Let $w$ be an array consisting only of $0$s and $1$s that is as long as there are vertices in $G$. Assume $w[x] = 1$ if the graph
    obtained from $G$ -- let us call it $H$ -- still has the $x$th vertex. It is easy to see that the probability of $H$ having those
    vertices denoted by $w$ is $\frac{1}{2^n}.$ Let $X(w)$ be the number of vertices that $H$ would have, were it represented by
    $w$. Let $X$ be the number of vertices $H$ has. \\
    
    Then $\bb{E}[X] = \sum_{w \in \Omega}^{} X(w) \frac{1}{2^n}$.  \\
    
    The right hand side of the inequality can be replaced by 
    $$\sum_{k=0}^{n} \underbrace{\text{ (The number of ways to get $k$ vertices multiplied by $k$ vertices) }}_{\alpha} \frac{1}{2^n}$$. Think about $\alpha$ for a second: for a given $k$ , $\alpha$ is really $\binom{n}{k} (k)$. \\

\end{problem}

\begin{problem}{Expected Edge Count}
    Suppose a graph has $n$ vertices and we randomly delete each vertex with probability $1/2$. The expected number of edges is: \\

    An edge $e$ is a connection between two vertices $u$ and $v$. Thus $e$ will continue to exist in $H$ if and only if $uv$ exists
    in $H$; that is, $u$ and $v$ must not be deleted, the probability of which is $1/4$. Let $E$ be the set of edges in $G$. Then
    the number of edges in $H$ is a random variable $X$ whose value is

    \[
        \sum_{(u,v) \in E}^{}X_[(u,v) \in E_H]
    \]

    where $X_{(u,v) \in E_H}$ is the binary random variable that takes on $1$ if $(u,v) \in E_H$ and $0$ otherwise. Its expectation is


    \[
        \sum_{(u,v) \in E}^{}\bb{P}[(u,v) \in E]
    \]

    which is just 

    \[
        \sum_{(u,v) \in E}^{}\frac{1}{4} = \frac{m}{4}
    \]
    
\end{problem}

\begin{outline}
    \1 Las Vegas algorithms are those algorithms that have a deterministic, correct ouput; however, these algorithms have a random running time depending on the input they're supplied with.
    \2 We are commonly interested in bounding their worst case expected running time. This quantity can be called $T(n)$, and it can be expressed as: 

    \[
        T(n) = \max_{X: \abs{X} = n} \bb{E}[X] \tag{$\alpha$}
    \]

    \1 Monte Carlo algorithms have a random output but a deterministic running time. We want to calculate what is the minimum probability that the algorithm produces an accurate output.
    \2 This can be expressed as 

    \[
        T(n) = \min_{X : \abs{X} = n } \bb{P}[f(X) \text{ ``is correct" }]
    \]
\end{outline}

\breathe \\
I now present two ways to analyze randomized quick sort. Recall that quick sort chooses a pivot in an array, sorts all elements in the array according to the pivot, and then recursively sorts the
``sorted'' halves. Let $Q(A)$ be the number of comparisons made
in order to quick sort $A$. Suppose $A_n$ has $n$ elements; then

\[
    Q(A_n) = \sum_{i=1}^{n} X_{i}\left( ( [Q(A_{i-1})] + [Q(A_{n-i})] \right)
\]

where $X_{i} = 1$ if $i$ is a pivot and $0$ otherwise.

\[
    Q(A_n) = \sum_{i=1}^{n} X_{i}\left( ( [Q(A_{i-1})] + [Q(A_{n-i})] \right)
\]

Now apply expectation to get:

\[
    \bb{E}[Q(A_n)] = \sum_{i=1}^{n}\bb{E}[X_{i}]\left( \bb{E}[Q(A_{i-1})] + \bb{E}[Q(A_{n-i})] \right) + n
\]

We skipped a step in that we jumped from $\bb{E}[Q(A_{j})X_{j+1}]$ to $\bb{E}[Q(A_{j})] \bb{E}[X_{j+1}]$. This is justified because $Q(A_{j})$ is independent of $X_{j+1}$; for the coin flips\footnote{We use coinflips to simulate a uniform distribution} used to choose pivot $j+1$ are independent of the coin flips that will be used in $A_{j}$.

\[
    \bb{E}[Q(A_n)] = \sum_{i=1}^{n}\frac{1}{n}\left( \bb{E}[Q(A_{i-1})] + \bb{E}[Q(A_{n-i})] \right) + n \tag{$\beta$}
\]


This recurrence is true for all inputs $A$ of size $n$ and, in particular, for the input that would require the maximum running time. Thus, if we define $T(n)$ as we did in $(\alpha)$, then the expression above becomes

\[
    T(n) \leq n + \sum_{i=1}^{n}\frac{1}{n}\left( T(i-1) + T(n-i) \right)
\]

Notice that we replaced $=$ with $\leq$, for the use of $T(n)$ means that we are working with maximums, and so the inequality in $\beta$ becomes less tight. For first (UNRESOLVED).

At this point, we can (UNRESOLVED) verify that this run time follows $O(n \log n)$ by running an inductive proof.

Another approach is to define $R_{ij}$ as the event that the $i$th rank element is compared with the $j$th rank element and to define $X_{ij}$ as the associated binary random variable. Thus, the total number of comparisons (and, hence, the running time of randomized quick sort) is given by 

\[
    X = \sum_{1 \leq i < j \leq n }^{}X_{ij}
\]

where $X$ is the total number of comparisons; whence

\[
    \bb{E}[X] = \sum_{1 \leq i < j \leq n }^{}\bb{E}[X_{ij}]
\]


\[
    \bb{E}[X] = \sum_{1 \leq i < j \leq n }^{}\bb{P}[R_{ij}]
\]

\begin{lemma}
    $\bb{P}[R_{ij}]$ is $\frac{2}{j-i+1}$.
\end{lemma}

\begin{proof}
    Identifying the $i$th through $j$th ranked elements as $\{i \dots j\}$, observe that $i$ is compared with $j$ if and only if the pivot chosen from among $\{i \dots j\}$ is either $i$ or $j$. Since pivot selection happens uniformly at random, there are two desireable values to choose from and $j - i + 1$ total values, the probability of $R_{ij}$ is as stated.
\end{proof}

Whence

\begin{align*}
    \bb{E}[X] &= \sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\frac{2}{j-i+1} \\
    &= \sum_{i=1}^{n-1}\sum_{\delta=2}^{n-i+1}\frac{2}{\delta} \\
    &= 2 \left(\sum_{i=1}^{n-1} H_{n-i+1} - 1\right)  \\
    &\leq  2 \left(\sum_{i=1}^{n-1} H_{n-i+1} \right)  \\
    &\leq  2 \left(\sum_{i=1}^{n-1} \log n \right)  \\
    &\leq  2 n \log n  \\
\end{align*}

Note that

\[
    \sum_{x=1}^{n}\frac{1}{x+1} \leq \underbrace{\int_{1}^{n}\left( \frac{1}{x} \right)}_{\ln(n)} \leq \sum_{x=1}^{n}\frac{1}{x}
\]

which establishes that $H(n) = \Theta(n)$, since both the left hand side and the right hand since


\[
    H_n = O(\sum_{x=1}^{n}\frac{1}{x+1}) \leq \underbrace{\int_{1}^{n}\left( \frac{1}{x} \right)}_{\ln(n)} \leq O(\sum_{x=1}^{n}\frac{1}{x}) = H_n
\]

\section{Lecture 3}{Probabilistic Inequalities}

\begin{remark}
    If we toss a fair coin $n$ times then the probability of getting $k$ heads is

    \[
        \binom{n}{k}(1/2^n)
    \]

    Recall that this holds, because any one sequence of heads and tails has a $\frac{1}{2^n}$
    probability of taking place; finally, there are $\binom{n}{k}$ such sequences that
    have $k$ heads.
\end{remark}

\begin{remark}
    Suppose we know that a random variable $Q$ satisfies $\bb{P}[Q \geq 10 n \log n] \leq c$. And we know that $Q \leq n^2$. Argue that $\bb{E}[Q] \leq 10 n \log n + (n^2 - 10 n \log n)c$.

    \begin{align*}
        \intertext{First note that} \\
        \bb{P}[Q < 10 n \log n] > 1 - c \\
        \intertext{It, therefore, follows that:} \\
        \bb{E}[Q] \leq n^2 \times (\bb{P}[Q \geq 10 n \log n]) + (\bb{P}[Q < 10 n \log n])(10 n \log n) \\
        = \bb{E}[Q] \leq n^2 \times (c) + (1 - c)(10 n \log n) \\
    \end{align*}

    This gives us what we desire.
\end{remark}

\begin{remark}
    Recall that if $X$ and $Y$ are independent, then $\text{Var}(X + Y) = \text{Var}(X) + \text{Vary}(Y)$ and that $\bb{E}[XY] = \bb{E}[X] \bb{E}[Y]$.
\end{remark}

\begin{remark}
    The Chebyshev bound is not, in certain cases, strong enough: \\

    Suppose that we simulate a one dimensional random walk; a random variable $X_{i}$
    takes on $-1$ at time $i$ if we move left at time $i$ and otherwise takes the value
    $1$. We wish to put bounds on the variable $Y = \sum_{i=1}^{n}X_{i}$. \\

    Observe that $\bb{E}[Y] = \sum_{i=1}^{n}\bb{E}[X_{i}] = 0$ and that, since all the
    $X_{i}$ are independent, 

    \[
        \var{Y} = \sum_{i=1}^{n} \var{X} = \sum_{i=1}^{n}\Ex{X^2} - \Ex{X}^2 \sum_{i=1}^{n}(1) = n
    \]

    Because of this, we can use Chebyshev's inequality. 

    \begin{align*}
        \prob{\abs{Y - \Ex{Y}} \geq t} \leq \frac{\sigma_x^2}{t^2} \\
        \prob{\abs{Y - \Ex{Y}} \geq t\sqrt{n}} \leq \frac{n}{nt^2} = \frac{1}{t^2} \\
        \prob{\abs{Y} \geq t\sqrt{n}} \leq \frac{n}{nt^2} = \frac{1}{t^2} \\
    \end{align*}
\end{remark}

\begin{center}
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{lecture3_chernoff_nonneg}
\end{center}

\begin{remark}
    The chernoff bound given before can be simpliefied in the case that $0 < \delta < 1$:

\begin{center}
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{lecture3_chernoff_nonneg_simp}
\end{center}

\end{remark}

\begin{remark}
    If we allow the random variables to take on negative values as well, then we find that
    the new bound depends on the number of variables (the dimension), whereas the theorem
    involving non-negative random variables was dimension free.

    \begin{center}
        \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{lecture3_chernoff_general}
    \end{center}


    Applying this new finding to the random walk example presened earlier, we see that
    we can revise the bounds to be

    \begin{align*}
        \prob{\abs{Y} \geq t\sqrt{n}} \leq 2 \exp{-t^2/2} = \frac{1}{t^2} \\
    \end{align*}

\end{remark}


\begin{example}
    Suppose that we toss $n$ balls into $n$ bins (or consider that we toss $m$ balls into $n$ bins). We wish to bound the random variable $Y$, which we set to the maximum number of balls in any bin. To do so, we employ the following procedure:

    \begin{outline}
        \1 Focus on one bin, and bound the probability that this one bin receives more than a certain number of balls.
        \2 Express this probability by first defining binary random variables that represent the probability that a ball falls into this chosen bin.
        \2 Use the chernoff bounds to define a bound (when we work forward, we will usually have to leave this bound expressed as a variable; at the end of the exercise, it will be clear what bound to set.
        \1 Apply the union bound to bound the probability that all bins receive more than a certain number of balls.
        \2 Use this to define the maximum number of balls that fall into any one bin.
        \1 Compute the expectation in a manner similar to the example shown earlier.
    \end{outline}

    Let $X_{ij}$ be the event that ball $j$ falls into bin $i$. Then set 

    \[
        X_{i} = \sum_{i=1}^{n}X_{ij}
    \]

    where $X_{i}$ represents the number of balls that fall into bin $i$. One Chernoff bound tells us that for $\delta > 0$, we have

    \[
        \prob{\abs{X_{i} > (1 + \delta)\mu } \leq (\frac{e^{\delta}}{(1 + \delta)^{1 + \delta}})^{\mu}
    \]

    In this case note that $\Ex{X_{i}} = \sum_{i=1}^{n}1 /n = 1$. \\

    Whence, this simplifies to.
    \[
        \prob{\abs{X_{i} > (1 + \delta) } \leq (\frac{e^{\delta}}{(1 + \delta)^{1 + \delta}})
    \]

    For a second (ie, we will be concrete about this later by choosing a suitable values for $\delta$), assume that

    \[
        (\frac{e^{\delta}}{(1 + \delta)^{1 + \delta}})^{\mu} < \frac{1}{n^3}
    \]

    Now observe that if we let $A_{i}$ be the event that $X_{i} > (1 + \delta)$

    \[
        \prob{\cu_{i=1}^{n}A_{i}} \leq \sum_{i=1}^{n}\prob{A_{i}} = 1/n^2
    \]

    It follows that
\end{example}



\end{document}
